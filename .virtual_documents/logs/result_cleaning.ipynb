import os
import re
import pandas as pd
import numpy as np
import json


pd.set_option('display.max_columns', 50)  # or 1000
pd.set_option('display.max_rows', 50)  # or 1000
pd.set_option('display.max_colwidth', 300)  # or 199


def get_start_idx(lines, substring):
    '''
    Obtain the beginning of the cleaning
    '''
    return [line_idx for line_idx, line in enumerate(lines) if substring in line]


LOGS_PATH = './logs'

def get_sliced_logs(lines):
    '''
    Slice the entire logs of multiple experiemnts into seperate experiements.
    '''
    
    # Step 1: Find the starting point.
    starter_ides = get_start_idx(lines, 'attention_head_mask')
    res = pd.DataFrame({'starter_ides': starter_ides})
    
    # Step 2: Base on all the start point, find the range
    res['ender_ides'] = res.apply(lambda x: x.shift(-1))
    res.iloc[-1, -1] = len(lines) # Upper bound of last row is the length!
    res['ender_ides'] = res['ender_ides'].astype('int')

    # Step 3: Obtain the corresponding lines base on the ranges
    sliced_lines = []
    for index, row in res.iterrows():
        starter_idx = row['starter_ides']
        ender_idx = row['ender_ides']
        sliced_lines.append(lines[starter_idx:ender_idx])
    return sliced_lines
        
def clean_log(logs):
    '''
    Clean the log of 1 experiment
    '''
    def clean_one_line(log_line):
        '''
        Clean 1 line of 1 log
        Examples
        --------
        >>> clean_one_line("00:31:00-INFO:   Batch size = 8")
        ('Batch size', '8')
        '''
        if '-INFO:   ' in log_line:
            result = log_line.split('-INFO:   ')
            if result:
                variable, value = result[1].split(' = ')
                value = re.findall(r'[-+]?(?:\d*\.\d+|\d+)', value)
                if value:
                    value = value[0]
                else:
                    value = None
                return variable, value
        
    variables = []
    values = []
    for log_line in logs:
        result = clean_one_line(log_line)
        if result:
            variable, value = result
            variables.append(variable)
            values.append(value)
    return variables, values

def get_train_time(header_logs):
    '''
    Examples
    --------
    >>> get_train_time("tot time 416.2169461250305 =========")
    416.2169461250305
    '''
    for header_log in header_logs:
        #print(header_log)
        if 'training time======' in header_log or (' =========' in header_log and 'tot ' in header_log) or ' training time======' in header_log:
            train_time = re.search(r'[-+]?(?:\d*\.\d+|\d+)', header_log).group()
            return train_time
    return None

def get_inference_time(header_logs):
    '''
    Examples
    --------
    >>> get_inference_time("evaluation time 0.7100062370300293")
    0.7100062370300293
    '''
    for header_log in header_logs:
        #print(header_log)
        if 'evaluation time' in header_log:
            train_time = re.search(r'[-+]?(?:\d*\.\d+|\d+)', header_log).group()
            return train_time
    return None
        
def get_experiment_result(task):
    with open(f'{LOGS_PATH}/head_pruning/{task}.txt') as f:
        lines = f.readlines()
    dfs = []
    for log in get_sliced_logs(lines):
        experiment = log[0].split(' ')[0]
        parameters = eval(log[1].replace('\n', ''))
        variables, values = clean_log(log[2:])
        # if task == 'rte':
        #     print(task, parameters, variables, values)
        df = pd.DataFrame({'task': task.lower(),
                           'experiments': experiment,
                           'drop_head_at_layer': int(parameters[0]),
                           'drop_head': int(parameters[1]),
                           'variables': variables,
                           'values': values})
        dfs.append(df)
    result = pd.concat(dfs, axis=0, ignore_index=True)
    result.columns.name = None
    result.loc[result['variables'].isin(['acc', 'eval_accuracy']), 'variables'] = 'accuracy'
    return result

def get_baseline_result(task):
    with open(f'{LOGS_PATH}/head_pruning/{task}.txt') as f:
        lines = f.readlines()
    header_start = min(get_start_idx(lines, 'Running evaluation'))
    header_end = min(get_start_idx(lines, 'attention_head_mask'))
    variables, values = clean_log(lines[header_start:header_end])
    train_time = get_train_time(lines[:header_start])
    if train_time:
        variables.append('train_time')
        values.append(train_time)
    inference_time = get_inference_time(lines)
    if inference_time:
        variables.append('inference_time')
        values.append(inference_time)
            
    result = pd.DataFrame({'task': task,
                           'variables': variables,
                           'values': values})
    result.loc[result['variables'].isin(['acc', 'eval_accuracy']), 'variables'] = 'accuracy'
    return result


experiment_results = []
baseline_results = []
for task in [task.replace('.txt', '') for task in os.listdir(LOGS_PATH + '/head_pruning') if '.ipynb_checkpoints' not in task]:
    baseline_results.append(get_baseline_result(task))
    experiment_results.append(get_experiment_result(task))
    
pd.concat(experiment_results, axis=0).to_csv('logs_cleaned/head_pruning_experiment_results.csv', index=False)
pd.concat(experiment_results, axis=0).to_pickle('logs_cleaned/head_pruning_experiment_results.pickle')
pd.concat(baseline_results, axis=0).to_csv('logs_cleaned/baseline_results.csv', index=False)
pd.concat(baseline_results, axis=0).to_pickle('logs_cleaned/baseline_results.pickle')


def get_logs_2(lines):
    logs = []
    start_ides = get_start_idx(lines, 'EXPERIMENT')
    for log_idx, strat_idx in enumerate(start_ides):
        if log_idx != len(start_ides) - 1:
            end_idx = start_ides[log_idx+1]
            log = lines[strat_idx:end_idx]
        else:
            log = lines[strat_idx:]
        logs.append(log)
    return logs

def get_experiment_result_2(task):
    with open(f'{LOGS_PATH}/layer_drop/{task}.txt') as f:
        lines = f.readlines()
    dfs = []
    for log in get_logs_2(lines):
        experiment = 'Remove Layers'
        parameter = log[0].split(' remove layers ')[1].replace('\n', '')
        variables, values = clean_log(log[3:])
        train_time = get_train_time(log)
        if train_time:
            variables.append('train_time')
            values.append(train_time)
        inference_time = get_inference_time(log)
        if inference_time:
            variables.append('inference_time')
            values.append(inference_time)
        df = pd.DataFrame({'task': task.lower(),
                           'experiments': experiment,
                           'parameter': parameter,
                           'variables': variables,
                           'values': values})
        dfs.append(df)
    result = pd.concat(dfs, axis=0, ignore_index=True)
    result.columns.name = None
    result.loc[result['variables'].isin(['acc', 'eval_accuracy']), 'variables'] = 'accuracy'
    return result

experiment_results_2 = []
for task in [task.replace('.txt', '') for task in os.listdir(LOGS_PATH + '/layer_drop') if '.ipynb_checkpoints' not in task]:
    experiment_results_2.append(get_experiment_result_2(task))
pd.concat(experiment_results_2, axis=0).to_csv('logs_cleaned/layer_drop_results.csv', index=False)
pd.concat(experiment_results_2, axis=0).to_pickle('logs_cleaned/layer_drop_results.pickle')


head_prune = pd.read_pickle('logs_cleaned/head_pruning_experiment_results.pickle')


benchmark_mapper = pd.DataFrame(
    {'task': ['sst-2', 'rte', 'mrpc', 'wnli', 'sts-b', 'cola'],
     'benchmark': ['accuracy', 'accuracy', 'F-1 score', 'accuracy', 'spearmanr', "Matthew's correlation"]}
)


head_prune_core_benchmark = head_prune.merge(benchmark_mapper, how='inner', on='task') \
    .query('variables == benchmark') \
    .drop(columns=['experiments', 'variables'])
head_prune_core_benchmark.to_csv('logs_cleaned/head_prune_core_benchmark.csv', index=False)


baseline = pd.read_pickle('logs_cleaned/baseline_results.pickle')
baseline_core_benchmark = baseline.merge(benchmark_mapper, how='inner', 
               left_on=['task', 'variables'],
               right_on=['task', 'benchmark']) \
    .drop(columns=['variables']) \
    .rename(columns={'values': 'baseline'})
baseline_core_benchmark['baseline'] = baseline_core_benchmark['baseline'].astype('double')
baseline_core_benchmark.to_csv('logs_cleaned/baseline_core_benchmark.csv', index=False)


head_prune_core_benchmark['values'] = head_prune_core_benchmark['values'].astype('double')


# Average
res = head_prune_core_benchmark \
    .merge(baseline_core_benchmark, how='left', on=['task', 'benchmark']) \
    .rename(columns={'values':'scores'}) \
    .assign(score_diff = lambda df: (df.scores - df.baseline) / df.baseline) \
    .groupby(["drop_head_at_layer", "drop_head"], as_index=False) \
    .agg(avg_glue = ('score_diff', 'mean')) 

res = res.pivot_table(index=['drop_head_at_layer'],
                values=['avg_glue'],
                columns=['drop_head'])

res.applymap(lambda row: str(round(row* 100, 2)) + '%')


# By task
def get_task_result(task, df = head_prune_core_benchmark):
    
    task_df = df[df['task'] == task]
    res = head_prune_core_benchmark \
        .merge(baseline_core_benchmark, how='left', on=['task', 'benchmark']) \
        .rename(columns={'values':'scores'}) \
        .assign(score_diff = lambda df: (df.scores - df.baseline) / df.baseline) \
        .groupby(["drop_head_at_layer", "drop_head"], as_index=False) \
        .agg(avg_glue = ('score_diff', 'mean')) 

    res = res.pivot_table(index=['drop_head_at_layer'],
                    values=['avg_glue'],
                    columns=['drop_head'])

    res = res.applymap(lambda row: str(round(row* 100, 2)) + '%')
    
    return res


get_task_result('mrpc')


get_task_result('sst-2')


get_task_result('cola')


get_task_result('sst-2')


get_task_result('sts-b')


get_task_result('rte')


benchmark_mapper = pd.DataFrame(
    {'task': ['sst-2', 'rte', 'mrpc', 'wnli', 'sts-b', 'cola'],
     'benchmark': ['accuracy', 'accuracy', 'F-1 score', 'accuracy', 'spearmanr', "Matthew's correlation"]}
)
layer_drop = pd.read_pickle('logs_cleaned/layer_drop_results.pickle')


# Clean the mixed parameters column
layer_drop['strategy'] = layer_drop['parameter'].apply(lambda x: re.search(r'\(([^()]+)\)', x).group(1))
layer_drop['n_layer_drop'] = layer_drop['strategy'].apply(lambda x: re.search(r'\d', x).group()).astype('int')
layer_drop['strategy'] = layer_drop['strategy'].apply(lambda x: re.sub(r' \d', '', x).replace('drop bottom', 'bottom drop').title())
layer_drop['layer_drop'] = layer_drop['parameter'].apply(lambda x: re.search(r'([^\(]+)', x).group(1))
layer_drop['values'] = layer_drop['values'].astype('double')
layer_drop = layer_drop.drop(columns = 'parameter')


layer_drop_core_benchmark = layer_drop.merge(benchmark_mapper, how='inner', on='task') \
    .query('variables == benchmark') \
    .drop(columns=['experiments', 'variables'])


layer_drop_core_benchmark.groupby('task').size()


layer_drop_core_benchmark = layer_drop.merge(benchmark_mapper, how='inner', on='task') \
    .query('variables == benchmark') \
    .drop(columns=['experiments', 'variables'])


layer_drop_core_benchmark.head()


layer_drop_core_benchmark.to_csv('logs_cleaned/layer_drop_core_benchmark.csv', index=False)


EPOCHS_mapper = pd.DataFrame(
    {'task': ['sst-2', 'rte', 'mrpc', 'wnli', 'sts-b', 'cola'],
     'n_epoch': [3, 10, 3, 2, 3, 3]}
)


TARGET_COL = ['train_time', 'inference_time', 'Num examples']
baseline_time = baseline. \
    query("variables in @TARGET_COL"). \
    pivot_table(index=['task'],
                values=['values'],
                columns=['variables']).reset_index(col_level=0)
baseline_time.columns = [col[0] for col in baseline_time.columns[:-len(TARGET_COL)]] + [col[1] for col in baseline_time.columns[-len(TARGET_COL):]]
baseline_time = baseline_time.merge(EPOCHS_mapper, on="task")
baseline_time['train_time_per_epoch_baseline'] = baseline_time['train_time'] / baseline_time['n_epoch']
baseline_time['inference_latency_baseline'] = baseline_time['inference_time'] / baseline_time['Num examples']
baseline_time['inference_throughput_baseline'] = baseline_time['Num examples'] / baseline_time['inference_time']
baseline_time = baseline_time[['task', 'train_time_per_epoch_baseline', 'inference_latency_baseline', 'inference_throughput_baseline']]
baseline_time


TARGET_COL = ['train_time', 'inference_time', 'Num examples']
layer_drop_time = layer_drop. \
    query("variables in @TARGET_COL"). \
    pivot_table(index=['task', 'experiments', 'strategy', 'n_layer_drop', 'layer_drop'],
                values=['values'],
                columns=['variables']).reset_index(col_level=0)#. \
layer_drop_time.columns = [col[0] for col in layer_drop_time.columns[:-len(TARGET_COL)]] + [col[1] for col in layer_drop_time.columns[-len(TARGET_COL):]]
layer_drop_time = layer_drop_time.merge(EPOCHS_mapper, on="task")
layer_drop_time['train_time_per_epoch'] = layer_drop_time['train_time'] / layer_drop_time['n_epoch']
layer_drop_time['inference_latency'] = layer_drop_time['inference_time'] / layer_drop_time['Num examples']
layer_drop_time['inference_throughput'] = layer_drop_time['Num examples'] / layer_drop_time['inference_time']
layer_drop_time = layer_drop_time.merge(baseline_time, on='task', how='inner')

for benchmark in ['train_time_per_epoch', 'inference_latency', 'inference_throughput']:
    layer_drop_time['diff_' + benchmark] = layer_drop_time[benchmark] - layer_drop_time[benchmark + '_baseline']
    layer_drop_time['percentage_diff_' + benchmark] = layer_drop_time['diff_' + benchmark] / layer_drop_time[benchmark]
layer_drop_time.head(5)


sts_b_base_latency = 7.414978265762329 / 1500


sts_b_drop2_latency = (9.799771785736084 + 9.799771785736084 + 10.391596794128418) / 4500


sts_b_drop2_latency / sts_b_base_latency


def get_speedup(df):
    df_output = df[['task', 'n_layer_drop']].iloc[0]
    df_output['Fine-tuning speedup'] = sum(df['train_time_per_epoch_baseline']) / sum(df['train_time_per_epoch'])
    df_output['Inference time save(%)'] = sum(df['inference_latency_baseline'] - df['inference_latency']) / sum(df['inference_latency_baseline'])
    df_output['Inference time save(%)'] = 100 * df_output['Inference time save(%)']
    return df_output
layer_drop_time.groupby(['task', 'n_layer_drop'], as_index=False).apply(get_speedup).reset_index(drop=True)
